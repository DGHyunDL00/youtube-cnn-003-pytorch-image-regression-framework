{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1eSTg5_z8M41iKexiulpMkJF9lwJPeL7k","authorship_tag":"ABX9TyNZWkBt6Oz28Cdcwm1lbz9m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8tuMA3kGT33M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0bd9a2dc-5b86-4e14-9cbd-e128f747fbfb"},"source":["!python \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/train.py\" \\\n","--data_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\" \\\n","--ckpt_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/inpainting/plain\" \\\n","--log_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/inpainting/plain\" \\\n","--result_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/inpainting/plain\" \\\n","--network \"unet\" \\\n","--task \"inpainting\" \\\n","--opts \"random\" 0.5 \\\n","--learning_type \"plain\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["learning rate: 1.0000e-03\n","batch size: 4\n","number of epoch: 100\n","data dir: /content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\n","ckpt dir: /content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/inpainting/plain\n","log dir: /content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/inpainting/plain\n","result dir: /content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/inpainting/plain\n","task: inpainting\n","opts: ['random', array([0.5])]\n","network: unet\n","learning type: plain\n","mode: train\n","device: cuda\n","2020-04-01 16:58:13.176177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0001 / 0050 | LOSS 0.4416\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0002 / 0050 | LOSS 0.4294\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0003 / 0050 | LOSS 0.3844\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0004 / 0050 | LOSS 0.3174\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0005 / 0050 | LOSS 0.2758\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0006 / 0050 | LOSS 0.2693\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0007 / 0050 | LOSS 0.2449\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0008 / 0050 | LOSS 0.2311\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0009 / 0050 | LOSS 0.2146\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0010 / 0050 | LOSS 0.1999\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0011 / 0050 | LOSS 0.1905\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0012 / 0050 | LOSS 0.1807\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0013 / 0050 | LOSS 0.1757\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0014 / 0050 | LOSS 0.1678\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0015 / 0050 | LOSS 0.1648\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0016 / 0050 | LOSS 0.1601\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0017 / 0050 | LOSS 0.1532\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0018 / 0050 | LOSS 0.1497\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0019 / 0050 | LOSS 0.1451\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0020 / 0050 | LOSS 0.1402\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0021 / 0050 | LOSS 0.1354\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0022 / 0050 | LOSS 0.1310\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0023 / 0050 | LOSS 0.1277\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0024 / 0050 | LOSS 0.1237\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0025 / 0050 | LOSS 0.1211\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0026 / 0050 | LOSS 0.1189\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0027 / 0050 | LOSS 0.1159\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0028 / 0050 | LOSS 0.1145\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0029 / 0050 | LOSS 0.1118\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0030 / 0050 | LOSS 0.1095\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0031 / 0050 | LOSS 0.1080\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0032 / 0050 | LOSS 0.1062\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0033 / 0050 | LOSS 0.1042\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0034 / 0050 | LOSS 0.1029\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0035 / 0050 | LOSS 0.1011\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0036 / 0050 | LOSS 0.1009\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0037 / 0050 | LOSS 0.0992\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0038 / 0050 | LOSS 0.0981\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0039 / 0050 | LOSS 0.0964\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0040 / 0050 | LOSS 0.0961\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0041 / 0050 | LOSS 0.0955\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0042 / 0050 | LOSS 0.0941\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0043 / 0050 | LOSS 0.0928\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0044 / 0050 | LOSS 0.0922\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0045 / 0050 | LOSS 0.0915\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0046 / 0050 | LOSS 0.0901\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0047 / 0050 | LOSS 0.0888\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0048 / 0050 | LOSS 0.0875\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0049 / 0050 | LOSS 0.0864\n","TRAIN: EPOCH 0001 / 0100 | BATCH 0050 / 0050 | LOSS 0.0852\n","VALID: EPOCH 0001 / 0100 | BATCH 0001 / 0025 | LOSS 0.0324\n","VALID: EPOCH 0001 / 0100 | BATCH 0002 / 0025 | LOSS 0.0313\n","VALID: EPOCH 0001 / 0100 | BATCH 0003 / 0025 | LOSS 0.0302\n","VALID: EPOCH 0001 / 0100 | BATCH 0004 / 0025 | LOSS 0.0298\n","VALID: EPOCH 0001 / 0100 | BATCH 0005 / 0025 | LOSS 0.0333\n","VALID: EPOCH 0001 / 0100 | BATCH 0006 / 0025 | LOSS 0.0343\n","VALID: EPOCH 0001 / 0100 | BATCH 0007 / 0025 | LOSS 0.0324\n","VALID: EPOCH 0001 / 0100 | BATCH 0008 / 0025 | LOSS 0.0342\n","VALID: EPOCH 0001 / 0100 | BATCH 0009 / 0025 | LOSS 0.0335\n","VALID: EPOCH 0001 / 0100 | BATCH 0010 / 0025 | LOSS 0.0323\n","VALID: EPOCH 0001 / 0100 | BATCH 0011 / 0025 | LOSS 0.0319\n","VALID: EPOCH 0001 / 0100 | BATCH 0012 / 0025 | LOSS 0.0319\n","VALID: EPOCH 0001 / 0100 | BATCH 0013 / 0025 | LOSS 0.0325\n","VALID: EPOCH 0001 / 0100 | BATCH 0014 / 0025 | LOSS 0.0320\n","VALID: EPOCH 0001 / 0100 | BATCH 0015 / 0025 | LOSS 0.0322\n","VALID: EPOCH 0001 / 0100 | BATCH 0016 / 0025 | LOSS 0.0316\n","VALID: EPOCH 0001 / 0100 | BATCH 0017 / 0025 | LOSS 0.0315\n","VALID: EPOCH 0001 / 0100 | BATCH 0018 / 0025 | LOSS 0.0314\n","VALID: EPOCH 0001 / 0100 | BATCH 0019 / 0025 | LOSS 0.0313\n","VALID: EPOCH 0001 / 0100 | BATCH 0020 / 0025 | LOSS 0.0309\n","VALID: EPOCH 0001 / 0100 | BATCH 0021 / 0025 | LOSS 0.0309\n","VALID: EPOCH 0001 / 0100 | BATCH 0022 / 0025 | LOSS 0.0309\n","VALID: EPOCH 0001 / 0100 | BATCH 0023 / 0025 | LOSS 0.0307\n","VALID: EPOCH 0001 / 0100 | BATCH 0024 / 0025 | LOSS 0.0310\n","VALID: EPOCH 0001 / 0100 | BATCH 0025 / 0025 | LOSS 0.0309\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0001 / 0050 | LOSS 0.0805\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0002 / 0050 | LOSS 0.0592\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0003 / 0050 | LOSS 0.0618\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0004 / 0050 | LOSS 0.0610\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0005 / 0050 | LOSS 0.0539\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0006 / 0050 | LOSS 0.0498\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0007 / 0050 | LOSS 0.0477\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0008 / 0050 | LOSS 0.0445\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0009 / 0050 | LOSS 0.0421\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0010 / 0050 | LOSS 0.0449\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0011 / 0050 | LOSS 0.0444\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0012 / 0050 | LOSS 0.0462\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0013 / 0050 | LOSS 0.0455\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0014 / 0050 | LOSS 0.0493\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0015 / 0050 | LOSS 0.0496\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0016 / 0050 | LOSS 0.0504\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0017 / 0050 | LOSS 0.0523\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0018 / 0050 | LOSS 0.0515\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0019 / 0050 | LOSS 0.0509\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0020 / 0050 | LOSS 0.0497\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0021 / 0050 | LOSS 0.0495\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0022 / 0050 | LOSS 0.0490\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0023 / 0050 | LOSS 0.0481\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0024 / 0050 | LOSS 0.0480\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0025 / 0050 | LOSS 0.0470\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0026 / 0050 | LOSS 0.0465\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0027 / 0050 | LOSS 0.0467\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0028 / 0050 | LOSS 0.0460\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0029 / 0050 | LOSS 0.0454\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0030 / 0050 | LOSS 0.0447\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0031 / 0050 | LOSS 0.0450\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0032 / 0050 | LOSS 0.0445\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0033 / 0050 | LOSS 0.0443\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0034 / 0050 | LOSS 0.0444\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0035 / 0050 | LOSS 0.0442\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0036 / 0050 | LOSS 0.0439\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0037 / 0050 | LOSS 0.0448\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0038 / 0050 | LOSS 0.0442\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0039 / 0050 | LOSS 0.0440\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0040 / 0050 | LOSS 0.0450\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0041 / 0050 | LOSS 0.0452\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0042 / 0050 | LOSS 0.0453\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0043 / 0050 | LOSS 0.0453\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0044 / 0050 | LOSS 0.0450\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0045 / 0050 | LOSS 0.0445\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0046 / 0050 | LOSS 0.0447\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0047 / 0050 | LOSS 0.0449\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0048 / 0050 | LOSS 0.0447\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0049 / 0050 | LOSS 0.0442\n","TRAIN: EPOCH 0002 / 0100 | BATCH 0050 / 0050 | LOSS 0.0444\n","VALID: EPOCH 0002 / 0100 | BATCH 0001 / 0025 | LOSS 0.0287\n","VALID: EPOCH 0002 / 0100 | BATCH 0002 / 0025 | LOSS 0.0281\n","VALID: EPOCH 0002 / 0100 | BATCH 0003 / 0025 | LOSS 0.0267\n","VALID: EPOCH 0002 / 0100 | BATCH 0004 / 0025 | LOSS 0.0256\n","VALID: EPOCH 0002 / 0100 | BATCH 0005 / 0025 | LOSS 0.0278\n","VALID: EPOCH 0002 / 0100 | BATCH 0006 / 0025 | LOSS 0.0289\n","VALID: EPOCH 0002 / 0100 | BATCH 0007 / 0025 | LOSS 0.0276\n","VALID: EPOCH 0002 / 0100 | BATCH 0008 / 0025 | LOSS 0.0297\n","VALID: EPOCH 0002 / 0100 | BATCH 0009 / 0025 | LOSS 0.0290\n","VALID: EPOCH 0002 / 0100 | BATCH 0010 / 0025 | LOSS 0.0279\n","VALID: EPOCH 0002 / 0100 | BATCH 0011 / 0025 | LOSS 0.0275\n","VALID: EPOCH 0002 / 0100 | BATCH 0012 / 0025 | LOSS 0.0274\n","VALID: EPOCH 0002 / 0100 | BATCH 0013 / 0025 | LOSS 0.0278\n","VALID: EPOCH 0002 / 0100 | BATCH 0014 / 0025 | LOSS 0.0274\n","VALID: EPOCH 0002 / 0100 | BATCH 0015 / 0025 | LOSS 0.0274\n","VALID: EPOCH 0002 / 0100 | BATCH 0016 / 0025 | LOSS 0.0270\n","VALID: EPOCH 0002 / 0100 | BATCH 0017 / 0025 | LOSS 0.0269\n","VALID: EPOCH 0002 / 0100 | BATCH 0018 / 0025 | LOSS 0.0269\n","VALID: EPOCH 0002 / 0100 | BATCH 0019 / 0025 | LOSS 0.0267\n","VALID: EPOCH 0002 / 0100 | BATCH 0020 / 0025 | LOSS 0.0264\n","VALID: EPOCH 0002 / 0100 | BATCH 0021 / 0025 | LOSS 0.0266\n","VALID: EPOCH 0002 / 0100 | BATCH 0022 / 0025 | LOSS 0.0267\n","VALID: EPOCH 0002 / 0100 | BATCH 0023 / 0025 | LOSS 0.0265\n","VALID: EPOCH 0002 / 0100 | BATCH 0024 / 0025 | LOSS 0.0266\n","VALID: EPOCH 0002 / 0100 | BATCH 0025 / 0025 | LOSS 0.0265\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0001 / 0050 | LOSS 0.0562\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0002 / 0050 | LOSS 0.0540\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0003 / 0050 | LOSS 0.0462\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0004 / 0050 | LOSS 0.0430\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0005 / 0050 | LOSS 0.0380\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0006 / 0050 | LOSS 0.0398\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0007 / 0050 | LOSS 0.0392\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0008 / 0050 | LOSS 0.0379\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0009 / 0050 | LOSS 0.0382\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0010 / 0050 | LOSS 0.0377\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0011 / 0050 | LOSS 0.0363\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0012 / 0050 | LOSS 0.0387\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0013 / 0050 | LOSS 0.0406\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0014 / 0050 | LOSS 0.0398\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0015 / 0050 | LOSS 0.0418\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0016 / 0050 | LOSS 0.0404\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0017 / 0050 | LOSS 0.0421\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0018 / 0050 | LOSS 0.0424\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0019 / 0050 | LOSS 0.0416\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0020 / 0050 | LOSS 0.0410\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0021 / 0050 | LOSS 0.0408\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0022 / 0050 | LOSS 0.0406\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0023 / 0050 | LOSS 0.0401\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0024 / 0050 | LOSS 0.0406\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0025 / 0050 | LOSS 0.0408\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0026 / 0050 | LOSS 0.0415\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0027 / 0050 | LOSS 0.0410\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0028 / 0050 | LOSS 0.0403\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0029 / 0050 | LOSS 0.0396\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0030 / 0050 | LOSS 0.0393\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0031 / 0050 | LOSS 0.0397\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0032 / 0050 | LOSS 0.0395\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0033 / 0050 | LOSS 0.0389\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0034 / 0050 | LOSS 0.0382\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0035 / 0050 | LOSS 0.0386\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0036 / 0050 | LOSS 0.0403\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0037 / 0050 | LOSS 0.0401\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0038 / 0050 | LOSS 0.0399\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0039 / 0050 | LOSS 0.0393\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0040 / 0050 | LOSS 0.0392\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0041 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0042 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0043 / 0050 | LOSS 0.0391\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0044 / 0050 | LOSS 0.0386\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0045 / 0050 | LOSS 0.0391\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0046 / 0050 | LOSS 0.0396\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0047 / 0050 | LOSS 0.0394\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0048 / 0050 | LOSS 0.0391\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0049 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0003 / 0100 | BATCH 0050 / 0050 | LOSS 0.0392\n","VALID: EPOCH 0003 / 0100 | BATCH 0001 / 0025 | LOSS 0.0219\n","VALID: EPOCH 0003 / 0100 | BATCH 0002 / 0025 | LOSS 0.0219\n","VALID: EPOCH 0003 / 0100 | BATCH 0003 / 0025 | LOSS 0.0204\n","VALID: EPOCH 0003 / 0100 | BATCH 0004 / 0025 | LOSS 0.0194\n","VALID: EPOCH 0003 / 0100 | BATCH 0005 / 0025 | LOSS 0.0200\n","VALID: EPOCH 0003 / 0100 | BATCH 0006 / 0025 | LOSS 0.0209\n","VALID: EPOCH 0003 / 0100 | BATCH 0007 / 0025 | LOSS 0.0199\n","VALID: EPOCH 0003 / 0100 | BATCH 0008 / 0025 | LOSS 0.0216\n","VALID: EPOCH 0003 / 0100 | BATCH 0009 / 0025 | LOSS 0.0211\n","VALID: EPOCH 0003 / 0100 | BATCH 0010 / 0025 | LOSS 0.0204\n","VALID: EPOCH 0003 / 0100 | BATCH 0011 / 0025 | LOSS 0.0203\n","VALID: EPOCH 0003 / 0100 | BATCH 0012 / 0025 | LOSS 0.0203\n","VALID: EPOCH 0003 / 0100 | BATCH 0013 / 0025 | LOSS 0.0208\n","VALID: EPOCH 0003 / 0100 | BATCH 0014 / 0025 | LOSS 0.0206\n","VALID: EPOCH 0003 / 0100 | BATCH 0015 / 0025 | LOSS 0.0207\n","VALID: EPOCH 0003 / 0100 | BATCH 0016 / 0025 | LOSS 0.0204\n","VALID: EPOCH 0003 / 0100 | BATCH 0017 / 0025 | LOSS 0.0203\n","VALID: EPOCH 0003 / 0100 | BATCH 0018 / 0025 | LOSS 0.0203\n","VALID: EPOCH 0003 / 0100 | BATCH 0019 / 0025 | LOSS 0.0202\n","VALID: EPOCH 0003 / 0100 | BATCH 0020 / 0025 | LOSS 0.0200\n","VALID: EPOCH 0003 / 0100 | BATCH 0021 / 0025 | LOSS 0.0201\n","VALID: EPOCH 0003 / 0100 | BATCH 0022 / 0025 | LOSS 0.0201\n","VALID: EPOCH 0003 / 0100 | BATCH 0023 / 0025 | LOSS 0.0199\n","VALID: EPOCH 0003 / 0100 | BATCH 0024 / 0025 | LOSS 0.0201\n","VALID: EPOCH 0003 / 0100 | BATCH 0025 / 0025 | LOSS 0.0201\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0001 / 0050 | LOSS 0.0385\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0002 / 0050 | LOSS 0.0353\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0003 / 0050 | LOSS 0.0346\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0004 / 0050 | LOSS 0.0356\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0005 / 0050 | LOSS 0.0334\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0006 / 0050 | LOSS 0.0307\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0007 / 0050 | LOSS 0.0330\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0008 / 0050 | LOSS 0.0334\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0009 / 0050 | LOSS 0.0327\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0010 / 0050 | LOSS 0.0355\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0011 / 0050 | LOSS 0.0342\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0012 / 0050 | LOSS 0.0353\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0013 / 0050 | LOSS 0.0356\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0014 / 0050 | LOSS 0.0365\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0015 / 0050 | LOSS 0.0401\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0016 / 0050 | LOSS 0.0399\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0017 / 0050 | LOSS 0.0393\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0018 / 0050 | LOSS 0.0383\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0019 / 0050 | LOSS 0.0399\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0020 / 0050 | LOSS 0.0400\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0021 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0022 / 0050 | LOSS 0.0391\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0023 / 0050 | LOSS 0.0389\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0024 / 0050 | LOSS 0.0383\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0025 / 0050 | LOSS 0.0380\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0026 / 0050 | LOSS 0.0378\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0027 / 0050 | LOSS 0.0377\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0028 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0029 / 0050 | LOSS 0.0384\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0030 / 0050 | LOSS 0.0382\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0031 / 0050 | LOSS 0.0382\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0032 / 0050 | LOSS 0.0386\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0033 / 0050 | LOSS 0.0384\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0034 / 0050 | LOSS 0.0395\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0035 / 0050 | LOSS 0.0403\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0036 / 0050 | LOSS 0.0399\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0037 / 0050 | LOSS 0.0400\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0038 / 0050 | LOSS 0.0395\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0039 / 0050 | LOSS 0.0400\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0040 / 0050 | LOSS 0.0397\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0041 / 0050 | LOSS 0.0396\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0042 / 0050 | LOSS 0.0394\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0043 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0044 / 0050 | LOSS 0.0391\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0045 / 0050 | LOSS 0.0392\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0046 / 0050 | LOSS 0.0392\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0047 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0048 / 0050 | LOSS 0.0389\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0049 / 0050 | LOSS 0.0392\n","TRAIN: EPOCH 0004 / 0100 | BATCH 0050 / 0050 | LOSS 0.0388\n","VALID: EPOCH 0004 / 0100 | BATCH 0001 / 0025 | LOSS 0.0246\n","VALID: EPOCH 0004 / 0100 | BATCH 0002 / 0025 | LOSS 0.0261\n","VALID: EPOCH 0004 / 0100 | BATCH 0003 / 0025 | LOSS 0.0231\n","VALID: EPOCH 0004 / 0100 | BATCH 0004 / 0025 | LOSS 0.0213\n","VALID: EPOCH 0004 / 0100 | BATCH 0005 / 0025 | LOSS 0.0215\n","VALID: EPOCH 0004 / 0100 | BATCH 0006 / 0025 | LOSS 0.0227\n","VALID: EPOCH 0004 / 0100 | BATCH 0007 / 0025 | LOSS 0.0215\n","VALID: EPOCH 0004 / 0100 | BATCH 0008 / 0025 | LOSS 0.0235\n","VALID: EPOCH 0004 / 0100 | BATCH 0009 / 0025 | LOSS 0.0230\n","VALID: EPOCH 0004 / 0100 | BATCH 0010 / 0025 | LOSS 0.0222\n","VALID: EPOCH 0004 / 0100 | BATCH 0011 / 0025 | LOSS 0.0220\n","VALID: EPOCH 0004 / 0100 | BATCH 0012 / 0025 | LOSS 0.0222\n","VALID: EPOCH 0004 / 0100 | BATCH 0013 / 0025 | LOSS 0.0229\n","VALID: EPOCH 0004 / 0100 | BATCH 0014 / 0025 | LOSS 0.0231\n","VALID: EPOCH 0004 / 0100 | BATCH 0015 / 0025 | LOSS 0.0232\n","VALID: EPOCH 0004 / 0100 | BATCH 0016 / 0025 | LOSS 0.0226\n","VALID: EPOCH 0004 / 0100 | BATCH 0017 / 0025 | LOSS 0.0225\n","VALID: EPOCH 0004 / 0100 | BATCH 0018 / 0025 | LOSS 0.0225\n","VALID: EPOCH 0004 / 0100 | BATCH 0019 / 0025 | LOSS 0.0224\n","VALID: EPOCH 0004 / 0100 | BATCH 0020 / 0025 | LOSS 0.0222\n","VALID: EPOCH 0004 / 0100 | BATCH 0021 / 0025 | LOSS 0.0222\n","VALID: EPOCH 0004 / 0100 | BATCH 0022 / 0025 | LOSS 0.0221\n","VALID: EPOCH 0004 / 0100 | BATCH 0023 / 0025 | LOSS 0.0219\n","VALID: EPOCH 0004 / 0100 | BATCH 0024 / 0025 | LOSS 0.0221\n","VALID: EPOCH 0004 / 0100 | BATCH 0025 / 0025 | LOSS 0.0222\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0001 / 0050 | LOSS 0.0198\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0002 / 0050 | LOSS 0.0528\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0003 / 0050 | LOSS 0.0432\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0004 / 0050 | LOSS 0.0383\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0005 / 0050 | LOSS 0.0452\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0006 / 0050 | LOSS 0.0439\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0007 / 0050 | LOSS 0.0471\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0008 / 0050 | LOSS 0.0447\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0009 / 0050 | LOSS 0.0445\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0010 / 0050 | LOSS 0.0425\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0011 / 0050 | LOSS 0.0417\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0012 / 0050 | LOSS 0.0415\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0013 / 0050 | LOSS 0.0407\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0014 / 0050 | LOSS 0.0430\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0015 / 0050 | LOSS 0.0426\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0016 / 0050 | LOSS 0.0418\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0017 / 0050 | LOSS 0.0410\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0018 / 0050 | LOSS 0.0396\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0019 / 0050 | LOSS 0.0394\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0020 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0021 / 0050 | LOSS 0.0381\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0022 / 0050 | LOSS 0.0375\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0023 / 0050 | LOSS 0.0374\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0024 / 0050 | LOSS 0.0374\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0025 / 0050 | LOSS 0.0368\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0026 / 0050 | LOSS 0.0367\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0027 / 0050 | LOSS 0.0367\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0028 / 0050 | LOSS 0.0368\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0029 / 0050 | LOSS 0.0383\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0030 / 0050 | LOSS 0.0380\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0031 / 0050 | LOSS 0.0380\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0032 / 0050 | LOSS 0.0378\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0033 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0034 / 0050 | LOSS 0.0403\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0035 / 0050 | LOSS 0.0404\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0036 / 0050 | LOSS 0.0405\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0037 / 0050 | LOSS 0.0401\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0038 / 0050 | LOSS 0.0403\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0039 / 0050 | LOSS 0.0399\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0040 / 0050 | LOSS 0.0398\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0041 / 0050 | LOSS 0.0398\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0042 / 0050 | LOSS 0.0396\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0043 / 0050 | LOSS 0.0390\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0044 / 0050 | LOSS 0.0395\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0045 / 0050 | LOSS 0.0412\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0046 / 0050 | LOSS 0.0414\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0047 / 0050 | LOSS 0.0417\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0048 / 0050 | LOSS 0.0420\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0049 / 0050 | LOSS 0.0415\n","TRAIN: EPOCH 0005 / 0100 | BATCH 0050 / 0050 | LOSS 0.0410\n","VALID: EPOCH 0005 / 0100 | BATCH 0001 / 0025 | LOSS 0.0284\n","VALID: EPOCH 0005 / 0100 | BATCH 0002 / 0025 | LOSS 0.0295\n","VALID: EPOCH 0005 / 0100 | BATCH 0003 / 0025 | LOSS 0.0279\n","VALID: EPOCH 0005 / 0100 | BATCH 0004 / 0025 | LOSS 0.0259\n","VALID: EPOCH 0005 / 0100 | BATCH 0005 / 0025 | LOSS 0.0280\n","VALID: EPOCH 0005 / 0100 | BATCH 0006 / 0025 | LOSS 0.0296\n","VALID: EPOCH 0005 / 0100 | BATCH 0007 / 0025 | LOSS 0.0293\n","VALID: EPOCH 0005 / 0100 | BATCH 0008 / 0025 | LOSS 0.0314\n","VALID: EPOCH 0005 / 0100 | BATCH 0009 / 0025 | LOSS 0.0304\n","VALID: EPOCH 0005 / 0100 | BATCH 0010 / 0025 | LOSS 0.0292\n","VALID: EPOCH 0005 / 0100 | BATCH 0011 / 0025 | LOSS 0.0290\n","VALID: EPOCH 0005 / 0100 | BATCH 0012 / 0025 | LOSS 0.0285\n","VALID: EPOCH 0005 / 0100 | BATCH 0013 / 0025 | LOSS 0.0287\n","VALID: EPOCH 0005 / 0100 | BATCH 0014 / 0025 | LOSS 0.0283\n","VALID: EPOCH 0005 / 0100 | BATCH 0015 / 0025 | LOSS 0.0279\n","VALID: EPOCH 0005 / 0100 | BATCH 0016 / 0025 | LOSS 0.0279\n","VALID: EPOCH 0005 / 0100 | BATCH 0017 / 0025 | LOSS 0.0279\n","VALID: EPOCH 0005 / 0100 | BATCH 0018 / 0025 | LOSS 0.0279\n","VALID: EPOCH 0005 / 0100 | BATCH 0019 / 0025 | LOSS 0.0278\n","VALID: EPOCH 0005 / 0100 | BATCH 0020 / 0025 | LOSS 0.0277\n","VALID: EPOCH 0005 / 0100 | BATCH 0021 / 0025 | LOSS 0.0286\n","VALID: EPOCH 0005 / 0100 | BATCH 0022 / 0025 | LOSS 0.0289\n","VALID: EPOCH 0005 / 0100 | BATCH 0023 / 0025 | LOSS 0.0286\n","VALID: EPOCH 0005 / 0100 | BATCH 0024 / 0025 | LOSS 0.0284\n","VALID: EPOCH 0005 / 0100 | BATCH 0025 / 0025 | LOSS 0.0282\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0001 / 0050 | LOSS 0.0477\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0002 / 0050 | LOSS 0.0669\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0003 / 0050 | LOSS 0.0536\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0004 / 0050 | LOSS 0.0483\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0005 / 0050 | LOSS 0.0449\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0006 / 0050 | LOSS 0.0431\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0007 / 0050 | LOSS 0.0393\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0008 / 0050 | LOSS 0.0373\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0009 / 0050 | LOSS 0.0361\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0010 / 0050 | LOSS 0.0387\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0011 / 0050 | LOSS 0.0394\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0012 / 0050 | LOSS 0.0382\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0013 / 0050 | LOSS 0.0370\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0014 / 0050 | LOSS 0.0370\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0015 / 0050 | LOSS 0.0391\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0016 / 0050 | LOSS 0.0383\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0017 / 0050 | LOSS 0.0369\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0018 / 0050 | LOSS 0.0355\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0019 / 0050 | LOSS 0.0355\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0020 / 0050 | LOSS 0.0344\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0021 / 0050 | LOSS 0.0337\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0022 / 0050 | LOSS 0.0334\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0023 / 0050 | LOSS 0.0335\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0024 / 0050 | LOSS 0.0334\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0025 / 0050 | LOSS 0.0331\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0026 / 0050 | LOSS 0.0329\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0027 / 0050 | LOSS 0.0325\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0028 / 0050 | LOSS 0.0321\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0029 / 0050 | LOSS 0.0317\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0030 / 0050 | LOSS 0.0315\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0031 / 0050 | LOSS 0.0310\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0032 / 0050 | LOSS 0.0315\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0033 / 0050 | LOSS 0.0310\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0034 / 0050 | LOSS 0.0306\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0035 / 0050 | LOSS 0.0304\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0036 / 0050 | LOSS 0.0307\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0037 / 0050 | LOSS 0.0306\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0038 / 0050 | LOSS 0.0310\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0039 / 0050 | LOSS 0.0307\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0040 / 0050 | LOSS 0.0308\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0041 / 0050 | LOSS 0.0312\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0042 / 0050 | LOSS 0.0309\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0043 / 0050 | LOSS 0.0315\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0044 / 0050 | LOSS 0.0312\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0045 / 0050 | LOSS 0.0311\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0046 / 0050 | LOSS 0.0312\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0047 / 0050 | LOSS 0.0329\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0048 / 0050 | LOSS 0.0325\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0049 / 0050 | LOSS 0.0322\n","TRAIN: EPOCH 0006 / 0100 | BATCH 0050 / 0050 | LOSS 0.0321\n","VALID: EPOCH 0006 / 0100 | BATCH 0001 / 0025 | LOSS 0.0191\n","VALID: EPOCH 0006 / 0100 | BATCH 0002 / 0025 | LOSS 0.0206\n","VALID: EPOCH 0006 / 0100 | BATCH 0003 / 0025 | LOSS 0.0188\n","VALID: EPOCH 0006 / 0100 | BATCH 0004 / 0025 | LOSS 0.0175\n","VALID: EPOCH 0006 / 0100 | BATCH 0005 / 0025 | LOSS 0.0184\n","VALID: EPOCH 0006 / 0100 | BATCH 0006 / 0025 | LOSS 0.0196\n","VALID: EPOCH 0006 / 0100 | BATCH 0007 / 0025 | LOSS 0.0188\n","VALID: EPOCH 0006 / 0100 | BATCH 0008 / 0025 | LOSS 0.0202\n","VALID: EPOCH 0006 / 0100 | BATCH 0009 / 0025 | LOSS 0.0198\n","VALID: EPOCH 0006 / 0100 | BATCH 0010 / 0025 | LOSS 0.0192\n","VALID: EPOCH 0006 / 0100 | BATCH 0011 / 0025 | LOSS 0.0191\n","VALID: EPOCH 0006 / 0100 | BATCH 0012 / 0025 | LOSS 0.0191\n","VALID: EPOCH 0006 / 0100 | BATCH 0013 / 0025 | LOSS 0.0195\n","VALID: EPOCH 0006 / 0100 | BATCH 0014 / 0025 | LOSS 0.0195\n","VALID: EPOCH 0006 / 0100 | BATCH 0015 / 0025 | LOSS 0.0194\n","VALID: EPOCH 0006 / 0100 | BATCH 0016 / 0025 | LOSS 0.0191\n","VALID: EPOCH 0006 / 0100 | BATCH 0017 / 0025 | LOSS 0.0190\n","VALID: EPOCH 0006 / 0100 | BATCH 0018 / 0025 | LOSS 0.0189\n","VALID: EPOCH 0006 / 0100 | BATCH 0019 / 0025 | LOSS 0.0189\n","VALID: EPOCH 0006 / 0100 | BATCH 0020 / 0025 | LOSS 0.0188\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CUEetMc2qtnm","colab_type":"code","colab":{}},"source":["!python \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/train.py\" \\\n","--data_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\" \\\n","--ckpt_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/inpainting/residual\" \\\n","--log_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/inpainting/residual\" \\\n","--result_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/inpainting/residual\" \\\n","--network \"unet\" \\\n","--task \"inpainting\" \\\n","--opts \"random\" 0.5 \\\n","--learning_type \"residual\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pq8sMLRSkjTK","colab_type":"code","colab":{}},"source":["!python \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/train.py\" \\\n","--data_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\" \\\n","--ckpt_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/denoising/plain\" \\\n","--log_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/denoising/plain\" \\\n","--result_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/denoising/plain\" \\\n","--network \"unet\" \\\n","--task \"denoising\" \\\n","--opts \"random\" 30.0 \\\n","--learning_type \"plain\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oa5XVigOklQ0","colab_type":"code","colab":{}},"source":["!python \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/train.py\" \\\n","--data_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\" \\\n","--ckpt_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/denoising/residual\" \\\n","--log_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/denoising/residual\" \\\n","--result_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/denoising/residual\" \\\n","--network \"unet\" \\\n","--task \"denoising\" \\\n","--opts \"random\" 30.0 \\\n","--learning_type \"residual\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPA8Jh8tOlxB","colab_type":"code","colab":{}},"source":["!python \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/train.py\" \\\n","--data_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\" \\\n","--ckpt_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/super_resolution/plain\" \\\n","--log_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/super_resolution/plain\" \\\n","--result_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/super_resolution/plain\" \\\n","--network \"unet\" \\\n","--task \"super_resolution\" \\\n","--opts \"bilinear\" 2.0 \\\n","--learning_type \"plain\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5mWoBsqOlta","colab_type":"code","colab":{}},"source":["!python \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/train.py\" \\\n","--data_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/datasets/BSR/BSDS500/data/images\" \\\n","--ckpt_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/checkpoint/super_resolution/residual\" \\\n","--log_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/log/super_resolution/residual\" \\\n","--result_dir \"/content/drive/My Drive/YouTube/youtube-cnn-003-pytorch-unet-autoencoder-regression/result/super_resolution/residual\" \\\n","--network \"unet\" \\\n","--task \"super_resolution\" \\\n","--opts \"bilinear\" 2.0 \\\n","--learning_type \"residual\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1pTLeE8agiTt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}